# Bigram Language Model
## Overview
This repository contains a Bigram Language Model that predicts the probability of the second letter in a bigram (a pair of consecutive characters) given the first letter. The model is useful for understanding and generating text based on the rules of a given language.

# Visualizing Bigram Probabilities
Included is a heatmap visualization that shows the probability distribution of the second letter in a bigram given the first letter. This visualization is a key feature for exploring the model's understanding of language structure.

![Bigram Probability Heatmap](probability-heatmap)

## Example
If we input the letter "t", the model will predict the next most probable letter to follow based on the dataset it was trained on. The resulting name is 'Tole'.

